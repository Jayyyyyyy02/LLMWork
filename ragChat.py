# ragChat.py
from langchain_openai import OpenAIEmbeddings
import os
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    print('api_keyì—†ìŒ')
    raise ValueError('api key ì—†ìŒ') #ì˜ˆì™¸ ë°œìƒ


embedding = OpenAIEmbeddings(api_key=api_key,
                            model='text-embedding-3-large')
# print('embedding: ',embedding)\

# í¬ë¡œë§ˆ ë””ë¹„
from langchain_chroma import Chroma
persist_directory = './chroma_store'
#ì²˜ìŒ ë§Œë“¤ ë•ŒëŠ” Chroma.from_documents(...,embedding,....)
#ê¸°ì¡´ ë§Œë“¤ì–´ì§„ í¬ë¡œë§ˆ ë¡œë”©ì‹œì—ëŠ” Chroma(...,embedding_function,...)
vector_store = Chroma(
    persist_directory = persist_directory,
    embedding_function = embedding
)
print('# ë²¡í„° ìŠ¤í† ì•„ ë¡œë”© ì„±ê³µ###')

# llm ì–¸ì–´ ëª¨ë¸
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model = "gpt-4o-mini", api_key = api_key)

# ë„íë¨¼íŠ¸ ì²´ì¸ ìƒì„±
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate

template="ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ contextì— ê¸°ë°˜í•´ì„œ ë‹µë³€í•˜ì„¸ìš”\n\n{context}"
qna_prompt = ChatPromptTemplate([
    SystemMessagePromptTemplate.from_template(template),
    MessagesPlaceholder(variable_name = "messages") #ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨
])

document_chain = create_stuff_documents_chain(llm,qna_prompt)
#ë¬¸ì„œ ì¡°ê°ì„ í•˜ë‚˜ë¡œ í•©ì³ì„œ llmì˜ contextì— ì§‘ì–´ë„£ê³ (ì±„ìš°ê³ ) ê²°ê³¼ë¥¼ ìƒì„±=>ë¬¸ì„œì²´ì¸ì„ ë§Œë“ ë‹¤

query_augumentation_prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template("""
        ë„ˆëŠ” ì§ˆë¬¸ ë³´ì • ì „ë¬¸ AIì•¼.
        ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³ í•´ ëª¨í˜¸í•œ ì§ˆë¬¸ì„ ëª…í™•íˆ ë°”ê¾¸ëŠ” ê²Œ ëª©ì ì´ì•¼.
        ëŒ€ëª…ì‚¬ë‚˜ ì´, ì €, ê·¸ì™€ ê°™ì€ í‘œí˜„ì„ ëª…í™•í•œ ëª…ì‚¬ë¡œ í‘œí˜„í•´
        ì´ì „ ëŒ€í™” ë§¥ë½ê³¼ ìƒê´€ì—†ì´, ìƒˆë¡œìš´ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ìƒ ë„ì‹œë‚˜ ì£¼ì œë¥¼ ëª…í™•í•˜ê²Œ íŒŒì•…í•˜ê³  ë³´ì •í•´. 
        ë³´ì •ëœ ì§ˆë¬¸ì´ ì›ë˜ ì§ˆë¬¸ì˜ ì˜ë„ì™€ ë‹¤ë¥´ê²Œ í•´ì„ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•´
                                            
        **ì ˆëŒ€ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•˜ì§€ ë§ê³ , ë³´ì •ëœ ì§ˆë¬¸ ë¬¸ì¥ í•˜ë‚˜ë§Œ ì¶œë ¥í•´.** 
        ì˜ˆì‹œ: ì„œìš¸ì˜ ë…¹ì§€ ê³µê°„ í™•ëŒ€ ê³„íšì€ ë¬´ì—‡ì¸ê°€ìš”?                                              
        """),    
    #MessagesPlaceholder(variable_name="messages"),
    # ê³¼ê±° ëŒ€í™” ì‚½ì… (ì´ì „ ëŒ€í™”ê°€ messagesë³€ìˆ˜ì— ë“¤ì–´ê°€ì„œ ëŒ€í™”ì˜ ë§¥ë½ì„ êµ¬ì„±)

    HumanMessagePromptTemplate.from_template("{query}")
    # ìƒˆë¡œìš´ ì§ˆë¬¸ (ë§ˆì§€ë§‰ìœ¼ë¡œ {query}ì— ìƒˆë¡œìš´ ì§ˆë¬¸ìœ¼ë¡œ ë“¤ì–´ê°)
])


query_augument_chain = query_augumentation_prompt|llm
#llmì´ ëª¨í˜¸í•œ ì§ˆë¬¸ì„ ëª…í™•í•˜ê²Œ ë°”ê¿”ì¤€ë‹¤

#ë¦¬íŠ¸ë¦¬ë²„ (ê²€ìƒ‰)
retriever = vector_store.as_retriever(k=3)
#k=3: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì´ìš©í•´ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ì¡°ê° 3ê°œë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •

# retriever: ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ kê°œ ì°¾ê¸°               
# query_augument_chain : ëª¨í˜¸í•œ ì§ˆë¬¸ì„ ëª…í™•í•˜ê³  ê²€ìƒ‰ì— ì í•©í•œ í˜•íƒœë¡œ ë³´ì •(ì¦ê°•)
# document_chain: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ contextë¡œ í™œìš©í•´ LLMì´ ìµœì¢… ë‹µë³€ ìƒì„± 

# ì±„íŒ… UI êµ¬í˜„
import streamlit as st
from langchain_core.messages import SystemMessage,HumanMessage,AIMessage

st.header("::ğŸ•LangChain Chatbot with RAG::")

if "messages" not in st.session_state:
    st.session_state['messages'] = [
        SystemMessage("ë„ˆëŠ” ë¬¸ì„œì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ëŠ” ë„ì‹œì •ì±… ì „ë¬¸ê°€ì•¼")
    ]

# í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    if isinstance(msg, SystemMessage):
        who = "system"
    elif isinstance(msg, AIMessage):
        who = "assistant"
    else:
        who = "user"
    st.chat_message(who).write(msg.content)

def get_ai_response(messages, docs):
    response = document_chain.stream({
        "messages":messages,
        "context":docs
    })
    #RAGê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ì–»ì–´ì˜´

    #ì „ì²´ë‹µë³€ì„ í•œ ë²ˆì— ë§Œë“¤ì§€ ì•Šê³ , ì¡°ê¸ˆì”© chunkë¡œ ìˆœì°¨ì ìœ¼ë¡œ ë‚´ë³´ë‚´ë‹¤=>ìŠ¤íŠ¸ë¦¬ë°
    for chunk in response:
        yield chunk 
        #yield chunk => ì‘ë‹µ ì¡°ê°ì„ ìˆœì°¨ì ìœ¼ë¡œ ë‚´ë³´ë‚¸ë‹¤


#ì‚¬ìš©ì ì…ë ¥ë°›ê¸°
if prompt := st.chat_input():
    st.chat_message("user").write(prompt)
    st.session_state.messages.append(HumanMessage(prompt))
    print("User: ", prompt)

    #ì‚¬ìš©ì ì…ë ¥í•œ ì§ˆë¬¸ì„ ì´ìš©í•´ í™•ì¥ëœ ì§ˆì˜ë¥¼ ë§Œë“¤ì
    augmented_query = query_augument_chain.invoke({
        #"messages": st.session_state["messages"],
        "query":prompt
    })
    print("augmented_query: ",augmented_query)
    st.info(f"ê²€ìƒ‰ìš© ì§ˆì˜ë¬¸ : {augmented_query}", icon="ğŸ’¡")

    print("="*70)
    print("ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰")
    print("="*70)
    docs = retriever.invoke(f"{prompt}\n{augmented_query}")
    #ë²¡í„° ë””ë¹„ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê°€ì ¸ì˜´

    for doc in docs:
        print(doc)
        print('-'*70)
        with st.expander(f"ë¬¸ì„œ: {doc.metadata.get('source',"ì•Œìˆ˜ ì—†ìŒ")}"):
            #íŒŒì¼ëª…ê³¼ í˜ì´ì§€ ì •ë³´ ì¶œë ¥
            st.write(f"page:{doc.metadata.get('page','')}")
            st.write(doc.page_content)
    print("="*70)

    #AI ë‹µë³€ ì¶œë ¥
    with st.spinner(f"AIê°€ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...{augmented_query}"):
        response = get_ai_response(st.session_state.messages, docs)
        result = st.chat_message('assistant').write_stream(response)
                #ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì¶œë ¥
        st.session_state['messages'].append(AIMessage(result))